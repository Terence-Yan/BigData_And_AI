#### 1.决策树(Decision Tree)概述
决策树是一类常见的机器学习方法。以二分类任务为例，我们希望从**给定训练数据集**学得一个模型用以**对新示例类进行分类**，这个把样本分类的任务，
可看做对“当前样本属于正类吗？”这个问题的“决策”或“判定”过程。顾名思义，决策树是**基于树形结构**来进行决策的，这恰是人类在面临决策问题时一种
很自然的处理机制。

#### 2.决策树的定义
一般的，一颗决策树包含一个**根节点**、若干个**内部节点**和若干个**叶节点**；叶节点对应于决策结果，其他每个节点则对应于一个**属性测试**；
每个节点包含的样本集合根据属性测试的结果被划分到子节点中；根节点包含**样本全集**。从根节点到每个叶节点的路径对应了一个**判定测试序列**。
决策树学习的目的是为了产生一棵**泛化能力强**，即处理未见示例能力强的决策树。

#### 3.决策树相关算法概述
* 在20世纪70年代后期和20世纪80年代初期，机器学习研究人员 J.Ross Quinlan 开发了决策树算法，称为**迭代的二分器**(Iterative Dichotomiser, **ID3**)。
这项工作扩展了 E.B.Hunt、J.Marin 和 P.T.Stone 的**概念学习系统**。Quinlan 后来提出了 **C4.5**(ID3的后继)，成为了新的监督学习算法的性能比较基准。
1984年，多位统计学家(L.Breiman、J.Friedman、R.Olshen 和 C.Stone)出版了著作《Classification and Regression Trees》(即 **CART**)，介绍了二叉
决策树的产生。ID3 和 CART大约同时独立地发明，但是从训练元组学习决策树却采用了类似的方法。这两个基础算法引发了决策树归纳研究的旋风。
* ID3、C4.5 和 CART都采用**贪心**(即非回溯的)方法，其中决策树以自顶向下递归的分治方式构造。大多数决策树归纳算法都**沿用这种自顶向下方法**，从训练
元组集和它们相关联的类标号开始构造决策树。随着树的构建，**训练集递归地划分成较小的子集**。

#### 4.决策树的分裂点的选择
决策树学习的关键是如何选择**最优划分属性**(即分裂点)。一般而言，随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点
的*“纯度”(purity)越来越高*。

#### 5.信息增益与ID3算法

#### 6.增益率与C4.5算法

#### 7.基尼指数与CART算法






