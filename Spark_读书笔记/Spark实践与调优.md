#### 1.应该用什么类型的硬件来搭建Spark集群？
Spark是一个内存(in-memory)计算网格，了解这一点很重要。因此，为实现效率的最大化，强烈建议把系统作为一个整体，在Spark框架内**留足内存**，以满足可能的
最大工作负载(或者数据集)的需要。并不是说不能在以后调整集群规模，但是若能提前规划会更好，对大公司来说尤其应该如此，因为在大公司里订单的采购往往需要
数周或数月的时间。

#### 2.Spark应用中内存的估计
这里有必要再强调一下内存的概念，当你计算所需要的内存大小时，要明白这种计算**并不是一对一**的关系。也就是说，对于给定的 1TB 数据集，实际需要的内存
不止 1TB。这是因为在Java里从一个数据集中创建对象时，这个对象通常比原来的数据元素大得多，是它的数倍。将这个膨胀倍数与所创建的对象数相乘，就能更准确
地估计出系统完成给定任务所需要的内存大小。

#### 3.Spark集群中，每个物理机应该设定多少个CPU核？
这个问题很难有完整的答案，因为一旦数据负载规范地加载到内存，应用程序通常会受到**网络或者CPU**的掣肘。这就是说，最简单的解决方案就是用一个相对小的
数据集来测试你的Spark程序，弄清楚到底是网络还是CPU在制约着程序，然后制定相应的方案。

#### 4.云的目的是为用户和机构提供可按需启用和按需扩展的机器集群。

#### 5.Spark适用的应用场景——ETL
Spark并不是所有任务都适用。因为Spark本质上是参照MapReduce范式设计的，它的长处是数据的**抽取、转换和加载**操作(Extract、Transform、Load，
三者合称为ETL)。这些处理方式通常被称为批处理——以分布式的方式高效地处理大量数据。**批处理的缺点是通常会引入较大的延迟**。尽管Spark开发人员花费
了很多精力来提升Spark Streaming模式，但是它依然只能做到**秒级**的计算。因此，对于真正的低延迟、高吞吐量的应用来说，Spark并不是一个合适的工具。
对于很多应用场景，Spark最擅长的还是处理典型的ETL工作负载。

#### 6.Spark中的主要资源
总的来说，在Spark生态系统中，主要关注三种类型的资源：磁盘存储、CPU和内存。

#### 7.Spark应用的并行度
当构建Spark应用时，把CPU核的数量和程序的并行度联系起来，或者与它能同时执行的任务关联起来，也是很有帮助的。Spark是建立在RDD上的，RDD是一种抽象，
它把分布式数据集看做一个包含多个分区的单一实体。在Spark中，**一个Spark任务(task)将在一个CPU核上处理一个RDD的一个分区**。因此，程序的并行度基本
上取决于数据的分区数以及可用的CPU核数。

#### 8.Spark中内存的分配方式
* 内存对所有Spark应用而言几乎都是至关重要的。通常来说，集群管理器分配内存的方式与分配CPU核这样的离散资源的方式是一样的。在集群中可用的内存总量被
分解成块(block)或者容器(container)，然后这些容器被分配给特定的应用。通过这种方式，集群管理器可以公平地分配内存和调度资源，避免进程被饿死。
* 大多数应用程序需要依据Spark程序中执行的**RDD转换**做某种程度的**调优**，来恰当地平衡内存需求。一个**内存配置不当**的Spark应用程序，运行起来
可能会很低效。

#### 9.内存动态自动调优功能
从Spark1.6开始，引入了内存动态自动调优功能。在1.6版本中，Spark会自动调整分配给shuffle操作和缓存的**内存比例**，同时也会调整分配的**内存总量**。
这样就能把更大的数据集装入到较小的内存中，而且无须对内存参数进行大量的调优，使编写程序更容易。

#### 10.Spark应用中的文件应该被切分成多大？
* 众所周知，在HDFS上每个文件都是按块存储的。当Spark读取这些文件的时候，**每个HDFS块会映射到一个Spark分区**(partition)。对每个分区，将启动一个
Spark任务(task)来读取和处理。如果你有足够的资源，而且数据分区得当，通常会给高并行计算带来很大好处。然而，任务太多会产生较高的调度开销，因此应当尽量
**避免不必要的调度**。总之，读取的文件数太多会导致启动的任务数相应增加，也会带来大量的任务调度开销。
* 除了大量的任务被启动，读取大量小文件也增加了**打开文件所带来的时间开销**。还有一点:所有文件路径都是在driver上处理的。如果文件包含很多小文件，
driver可能面临内存压力。
* 另一方面，如果数据集是由一组庞大的文件组成，必须确保这些文件是可切分的；否则，必定由单独的任务来处理这些文件，这将需要一个非常大的分区，也会大大
降低程序性能。

#### 11.文件的压缩与可切分
一个压缩文件是否支持可切分，不仅取决于压缩编解码器，还取决文件的格式。如对支持块结构的文件格式，如Sequence文件或ORC文件，使用不可切分的编解码器，
那么每个块都会被压缩。在这种情况下，Spark会对每一个块并行启动任务。所以，你可以认为它们是可分的。但是，另一方面，如果用它们压缩文本文件，那么整个
文件会被压缩到一个块中，因此对于每个文件都会有一个任务被启动。

#### 12.Spark中JSON文件的处理
* 对JSON文件的读取和处理，SparkSQL有一个专门的方法。其中一个好处是可以让SparkSQL根据数据集推断或者通过编程方式指定schema。如果你提前知道了
schema，建议提供出来，省得Spark**再次扫描整个输入文件去确定schema**。这种方式的另一个好处是允许你自己**决定需要处理的字段**。如果JSON文件
中有很多你并不需要的字段，你可以仅指定相关的字段，其它的将会被忽略掉。
```
   val schema = new StructType(Array(new StructField("name", StringType, false), new StructField("age", IntegerType, false)))
   val specifiedSchema = sqlContext.jsonField("file.json", schema)
```
* 上面处理JSON文件的方式假设**每一行都有一个JSON对象**。如果一些JSON对象缺失一些字段，那么这些字段会被默认替换为null值。在推断schema时，
如果有一些错误的输入，SparkSQL会创建一个名为_corrupt_record的新列。这些错误的输入会在这一列中存储它们的数据，而其它列都为null值。

#### 13.Sequence文件
Sequence文件是一种常用的文件格式，由**二进制键值对**组成，这些键值对必须是Hadoop **Writable接口**的子类。因为有**同步标记的特性**，它们
在分布式处理中很受欢迎。有这些特性，你就能找到记录的边界来做并行处理。**Sequence文件是一种十分高效的数据存储格式，因为它能被高效地压缩及解压**。

#### 14.Avro文件
Avro文件格式是一种依赖于schema的二进制数据格式。当以Avro格式存储数据时，**schema总是与数据一起存储**。这个特点使得在不同的应用程序中都可以读取
Avro文件。

#### 15.Parquet文件
* Parquet文件格式是一种**支持嵌套数据结构的列式文件格式**。列式存储格式非常适用于**聚合查询**，因为从磁盘中读取数据时仅返回需要的列。Parquet文件
支持高效地压缩和编码schema，因为它们可以按列指定。这正是使用这种文件格式能减少磁盘I/O操作、节省更多存储空间的原因。
* SparkSQL提供专门的方法来读写保存数据schema的Parquet文件，并且Parquet文件格式**支持schema演化**，起初可以只有几列，然后按需添加更多的列。
Parquet会自动检测这些schema差异并自动合并。不过，在非必要情况下应避免schema合并，因为**该操作严重影响性能**。下面的例子演示了如何读取
Parquet文件并启用schema合并功能：
```
  val parquetDF = sqlContext.read.option("mergeSchema","true").parquet("parquetFolder")
```
* 在SparkSQL中，Parquet Datasource能够检测数据是否已分区并确定分区。这对于数据分析是一项重要的优化，因为在一次查询中，只有需要的分区才会根据查询
语句中的断言(predicate)被扫描。
* 从SparkSQL最佳实践的角度，鼓励使用Parquet文件格式。























