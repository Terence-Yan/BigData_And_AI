#### 1.应该用什么类型的硬件来搭建Spark集群？
Spark是一个内存(in-memory)计算网格，了解这一点很重要。因此，为实现效率的最大化，强烈建议把系统作为一个整体，在Spark框架内**留足内存**，以满足可能的
最大工作负载(或者数据集)的需要。并不是说不能在以后调整集群规模，但是若能提前规划会更好，对大公司来说尤其应该如此，因为在大公司里订单的采购往往需要
数周或数月的时间。

#### 2.Spark应用中内存的估计
这里有必要再强调一下内存的概念，当你计算所需要的内存大小时，要明白这种计算**并不是一对一**的关系。也就是说，对于给定的 1TB 数据集，实际需要的内存
不止 1TB。这是因为在Java里从一个数据集中创建对象时，这个对象通常比原来的数据元素大得多，是它的数倍。将这个膨胀倍数与所创建的对象数相乘，就能更准确
地估计出系统完成给定任务所需要的内存大小。

#### 3.Spark集群中，每个物理机应该设定多少个CPU核？
这个问题很难有完整的答案，因为一旦数据负载规范地加载到内存，应用程序通常会受到**网络或者CPU**的掣肘。这就是说，最简单的解决方案就是用一个相对小的
数据集来测试你的Spark程序，弄清楚到底是网络还是CPU在制约着程序，然后制定相应的方案。

#### 4.云的目的是为用户和机构提供可按需启用和按需扩展的机器集群。

#### 5.Spark适用的应用场景——ETL
Spark并不是所有任务都适用。因为Spark本质上是参照MapReduce范式设计的，它的长处是数据的**抽取、转换和加载**操作(Extract、Transform、Load，
三者合称为ETL)。这些处理方式通常被称为批处理——以分布式的方式高效地处理大量数据。**批处理的缺点是通常会引入较大的延迟**。尽管Spark开发人员花费
了很多精力来提升Spark Streaming模式，但是它依然只能做到**秒级**的计算。因此，对于真正的低延迟、高吞吐量的应用来说，Spark并不是一个合适的工具。
对于很多应用场景，Spark最擅长的还是处理典型的ETL工作负载。

#### 6.Spark中的主要资源
总的来说，在Spark生态系统中，主要关注三种类型的资源：磁盘存储、CPU和内存。

#### 7.Spark应用的并行度
当构建Spark应用时，把CPU核的数量和程序的并行度联系起来，或者与它能同时执行的任务关联起来，也是很有帮助的。Spark是建立在RDD上的，RDD是一种抽象，
它把分布式数据集看做一个包含多个分区的单一实体。在Spark中，**一个Spark任务(task)将在一个CPU核上处理一个RDD的一个分区**。因此，程序的并行度基本
上取决于数据的分区数以及可用的CPU核数。

#### 8.Spark中内存的分配方式
* 内存对所有Spark应用而言几乎都是至关重要的。通常来说，集群管理器分配内存的方式与分配CPU核这样的离散资源的方式是一样的。在集群中可用的内存总量被
分解成块(block)或者容器(container)，然后这些容器被分配给特定的应用。通过这种方式，集群管理器可以公平地分配内存和调度资源，避免进程被饿死。
* 大多数应用程序需要依据Spark程序中执行的**RDD转换**做某种程度的**调优**，来恰当地平衡内存需求。一个**内存配置不当**的Spark应用程序，运行起来
可能会很低效。

#### 9.内存动态自动调优功能
从Spark1.6开始，引入了内存动态自动调优功能。在1.6版本中，Spark会自动调整分配给shuffle操作和缓存的**内存比例**，同时也会调整分配的**内存总量**。
这样就能把更大的数据集装入到较小的内存中，而且无须对内存参数进行大量的调优，使编写程序更容易。

#### 10.Spark应用中的文件应该被切分成多大？
* 众所周知，在HDFS上每个文件都是按块存储的。当Spark读取这些文件的时候，**每个HDFS块会映射到一个Spark分区**(partition)。对每个分区，将启动一个
Spark任务(task)来读取和处理。如果你有足够的资源，而且数据分区得当，通常会给高并行计算带来很大好处。然而，任务太多会产生较高的调度开销，因此应当尽量
**避免不必要的调度**。总之，读取的文件数太多会导致启动的任务数相应增加，也会带来大量的任务调度开销。
* 除了大量的任务被启动，读取大量小文件也增加了**打开文件所带来的时间开销**。还有一点:所有文件路径都是在driver上处理的。如果文件包含很多小文件，
driver可能面临内存压力。
* 另一方面，如果数据集是由一组庞大的文件组成，必须确保这些文件是可切分的；否则，必定由单独的任务来处理这些文件，这将需要一个非常大的分区，也会大大
降低程序性能。

#### 11.文件的压缩与可切分
一个压缩文件是否支持可切分，不仅取决于压缩编解码器，还取决文件的格式。如对支持块结构的文件格式，如Sequence文件或ORC文件，使用不可切分的编解码器，
那么每个块都会被压缩。在这种情况下，Spark会对每一个块并行启动任务。所以，你可以认为它们是可分的。但是，另一方面，如果用它们压缩文本文件，那么整个
文件会被压缩到一个块中，因此对于每个文件都会有一个任务被启动。







