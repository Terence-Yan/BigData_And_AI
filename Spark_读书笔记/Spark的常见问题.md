#### 1.在Windows环境安装完Spark之后，在启动scala的shell客户端时，会碰到关于“/tmp/hive 目录权限不够”的问题
解决方案：查看Spark安装的根目录(即位于哪一个磁盘下,如D:\)是否存在tmp目录,如存在，将其删除或重命名即可，即安装Spark时，其安装根目录下不能有tmp目录。

#### 2.在DOS命令行执行spark-shell2的命令时，会出现“此时不应有 =true”的字符串提示信息的问题
解决方案：查看scala的环境变量配置信息,保证其安装目录不能有空格。
