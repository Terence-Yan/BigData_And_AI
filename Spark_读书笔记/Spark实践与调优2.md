#### 1.Spark执行模式
* 当运行一个Spark应用时，driver进程会随着集群worker节点上的一系列executor进程一起启动。driver负责运行用户的应用程序，当有action被触发时driver负责
管理所需执行的所有工作。另一方面，executor进程以任务(task)的形式执行实际的工作以及保存结果。但是，这些任务是如何分配给executor的呢？
* 对于Spark应用内部触发的**每个action**，DAG调度器都会创建**一个执行计划**来完成它。执行计划就是将尽可能多的窄依赖(narrow dependency)转换
(transformation)装配到各步骤(stage)中。RDD间的**窄依赖**是指父RDD的每一个分区最多能被一个子RDD的分区使用。当有一些**宽依赖需要做shuffle操作**时，
stage就受限制了。当多个子RDD的分区使用同一个父RDD的分区时，RDD间就会产生**宽依赖**。

#### 2.分区
分区其实就是RDD中的数据被切分后形成的片段。当DAG调度器将job转换为stage时，每个分区将被处理成一个task，每个task需要一个CPU核来执行。这意味着Spark
应用的**并行度取决于RDD的分区数**。因此，不难理解对Spark应用性能进行**调优**时，**RDD的分区数**可能是需要考虑的**最重要**的事情。

#### 3.RDD：一个只读且分区的记录集。

#### 4.RDD的分区数与其创建方式
* RDD的分区数与其创建方式高度相关。从**文件**创建的RDD都有**默认**的分区数。例如，如果文件存储在HDFS上，分区数将等于文件块数目(一个文件块对应
一个分区)。这意味着可以通过在HDFS上写文件时的块大小，或者通过配置InputFormat创建的分片(split)的多少，来控制分区数。
* 也可以通过并行化集合来创建RDD。其默认的分区数是由spark.default.parallelism属性决定的。这个默认值由集群管理器决定：对于运行在local模式的
Spark1.5.2来说，其值为CPU核的数目；对于细粒度模式的Mesos来说，其值为8；在其他情况下，分区数取2与所有executor上的CPU核总数的最大值。
* 但是，你可以控制这些默认值。对这两种创建RDD的方式，可以通过一个用户输入参数来控制分区的数量：
```
   sc.textFile(<inputPath>,<minPartitions>)
   sc.parallelize(<sequence>,<numSlices>)
```

#### 5.转换操作得到的RDD的分区数
* 最常见的创建RDD的方式，是对已有的RDD进行一些转换操作(transformation)。通常，一个RDD的分区数与它**所依赖的RDD分区数相同**。然而，有一些转换，例如
“union”(合并)就不受此规则的制约，因为它创建的RDD的分区数等于父RDD所有分区的总和。
* 另一种会引起数据shuffle的转换操作。这类转换都是宽依赖，即计算RDD的一个分区需要处理父RDD的多个分区的数据。这种情况下，如果不特别指定，默认分区数将是
所依赖的RDD的最大分区数。

#### 6.高效与最优分区数
* 要想写一个高效的Spark应用程序，就必须设置最优的分区数。假设job生成的任务数少于可用的CPU核数。这种情况下，可能面临两个性能问题：第一，不能充分发挥
整体计算能力；第二，如果分区数量少，单个分区的数据量将会比分区数量更多时大很多。对于更大的数据集，执行任务时还会有内存压力和垃圾回收的压力，导致运算
速度减慢。
* 同样，如果一个分区内的数据太大无法加载到内存，数据将不得不溢写到磁盘以避免出现OOM异常。但是溢写到磁盘需要排序及磁盘I/O等操作，会带来巨大的开销。
* 为充分利用集群的计算能力，分区数至少应当等于集群分配给应用程序的CPU数，但是分区过大的问题依然没有得到解决。如果你的数据集非常大，而集群又相当小，
那么你的分区还是会过大。这种情形下，RDD的分区数必须远高于可用的CPU核数。
* 另一方面，你还得考虑周全以防落入另一种极端：**分区数过多**。分区数过多将会生成许多需要发送到worker节点执行的小任务，这将增加任务调度的开销。
不过，启动任务所带来的性能损失比数据溢写到磁盘的损失要小。如果有许多任务几乎瞬时完成，或者这些任务根本没有执行任何读写操作，这就表明你的任务并行
度太高了。
* 对于RDD来说，很难计算出一个最佳的分区数，因为这很大程度上取决于数据集的大小、分区器本身，以及每个任务可用的总内存。为估算出一个比较精确的分区数，
你需要了解你的数据及其分布情况。不过，建议把每个RDD的分区数设置为**CPU数的2到4倍**。

#### 7.Spark Streaming中的GC
GC在Spark Streaming应用中扮演了很重要的角色，因为这些应用要求**低延时**，所以GC产生的较长时间的停顿是很讨厌的。在这种特别情形下，为了让GC停顿时间
一直保持在低值，推荐在**driver和executor**上使用并发 **mark-and-sweep** (标记-清除算法)的GC。





