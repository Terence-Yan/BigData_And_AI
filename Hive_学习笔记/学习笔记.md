#### 1.Hive的本质
```
hive的本质是，在hive数据表与已经存在的结构化的数据文件之间建立映射关系。映射建立成功之后，
就可以通过SQL来分析这些结构化的数据文件，避免了写MR程序。
```
#### 2.Hive数据库系统与文件的对应关系
```
hive数据库       对应    /usr/hive/warehouse(HDFS)目录下的一个文件夹
    数据表       对应    所在数据库的目录下的子文件夹
表中的数据       对应    所在数据表目录下的文件
```

#### 3.Hive的建表语句
```
创建hive表时，可能还需要指定分隔符，否则数据可能会加载不成功：
   create table test(id int, name string , age int) row format delimited fields terminated by ',';
   其中，表名--test，字段--id,name,age。
   delimited关键字表示使用的是内置分隔符。
```

#### 4.复杂数据类型指定分隔符
```
HQL建表语句：
create table t_hobby(id int,name string,hobby map<string, string>) 
row format delimited fields terminated by ',' 
collection items terminated by '-' 
map keys terminated by ':';
示例数据：
   1,Tom,打游戏:非常喜欢-篮球:喜欢
   2,Lucy,唱歌:非常喜欢-跳舞:喜欢-游泳:一般
```

#### 5.建表注意事项
```
1.建表的时候一定要根据所对应的结构化数据文件的分隔符来指定建表分隔符；
2.建表的字段的个数及字段类型要与对应的结构化数据文件中的数据格式匹配。
```

#### 6.关于Hive的默认分隔符
```
1.hive建表的时候默认的分隔符是'\001'，若在建表的时候没有指定分隔符，load文件的时候文件的分隔符就被默认为是'\001';若文件分隔符
不是'\001'，程序不会报错，但表查询的结果会全部是“NULL”。
2.用vi编辑器，先Ctrl+v,然后Ctrl+a即可输入'\001'-------^A
```

#### 7.Hive读写文件的机制
```
1.Hive读取文件的机制：首先调用InputFormat(默认TextInputFormat)，返回一条一条的记录(默认是一行对应一条记录)。然后调用
  SerDe(默认是LazySimpleSerDe)的Deserializer，将一条记录切分为各个字段(默认分隔符--'\001')。
2.Hive写文件的机制：将Row写入文件时，主要调用OutputFormat、SerDe的Serializer,顺序与读取相反。
```

#### 8.Partitioned By与分区表
```
1.在Hive的select查询中一般会扫描整个表的内容，会消耗很多时间做没有必要的工作。有时候只需扫描表中关心的某一特定部分的数据，因此
  hive在建表时,通过关键字partitioned by 引入了分区(partition)的概念。
2.分区表指的是在创建表的时候指定的partition的分区空间。一个表可以拥有一个或多个分区，每个分区以文件夹的形式单独存在于所属表文件夹的
  目录下。表和列名不区分大小写。分区是以字段的形式在表结构中存在，通过describe table 命令可以查看到该字段存在，但是该字段不存放实际
  的数据内容，仅仅是分区的表示。
3.HQL建表语句示例：create table t_employee(id int,name string) partitioned by(country string) 
  row format delimited fields terminated by ',';
  对应数据文件的数据格式：
  1,Tony
  2,李明
4.注意事项：a).用于分区的字段不能是数据表中已经存在的字段；b).分区字段是一个虚拟字段，不存放任何数据；c).分区字段的值是在装载表的
  分区数据时指定的：Load DATA Local inpath '/usr/hive/a.txt' into table test partition(country='USA'); . 
5.多分区表，HQL建表语句示例：create table t_employee(id int,name string) partitioned by(day string,hour string) 
  row format delimited fields terminated by ',';
6.分区表字段在HDFS上的体现是所属数据表的文件夹目录下会依据分区字段依次建立子文件夹，建立规则是左边的分区字段所对应的文件夹目录
  是相邻的右边的分区字段所对应文件夹的父目录。
```

#### 9.Clustered By...INTO num_buckets BUCKETS与分桶表
```
1.创建分桶表之前，应该先开启分桶功能，默认是关闭的。
  指定开启分桶功能： set hive.enforce.bucketing=true;
                   set mapreduce.job.reduces=4;
  HQL建表语句示例：create table t_stu(sNo int,sName string,sSex string,sAge int,sDept string) clustered by(sNo) 
  into 4 buckets
  row format delimited fields terminated by ',';
2.分桶表(分簇表)创建的时候，分桶字段必须是数据表中已定义的字段
3.不能使用load data方式导入分桶数据
4.加载分桶数据的HQL语句示例：insert overwrite table t_stu select * from student clustered by(sNo);
  导入分桶数据步骤：1.创建中间结果表student，将原始文件数据装载进该表；2.通过insert...select...语句将分桶后的数据加载进目标表t_stu。
5.分桶对应的是MapReduce任务中的partitioner.
6.分桶表体现在对应的结构化数据文件上是，将其所对应的一个数据文件拆分成更小的几个数据文件。
```

#### 10.对数据分桶的目的与作用
* 对于每一个表或者分区，Hive可以进一步组织成桶，也就说桶是更为细粒度的数据范围划分。Hive也是针对某一列进行分桶的组织。Hive采用对列值哈希，
然后除以桶的个数再求余的方式决定该条记录存放在哪个桶当中。
* 把表或分区组织成桶有两个理由：</br>
  (1).获得更高的查询处理效率。桶为表加上了额外的结构，Hive在处理有些查询时能利用这个结构。具体而言，连接两个在(包含连接的列)相同列上划分了桶的表，
  可以使用Map 端连接(Map-side join)高效的实现，比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作，那么将保存相同列值
  的桶进行JOIN操作就可以，可以大大减少JOIN的数据量。</br>
  (2).使取样(sampling)更高效。在处理大规模数据集时，在开发和修改查询阶段，如果能在数据集的一小部分上试运行查询，会带来很多方便。
  
#### 11.EXTERNAL关键字与外部表
* EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的存储路径(通过LOCATION关键字指定).
  HQL创建外部表语句示例：
  ```
  create external table t_stu(sNo int,sName string,sSex string,sAge int,sDept string)    
  row format delimited fields  terminated by ',' location '/tmp/hive';
  注：location指定的必须是HDFS下的路径，建表时hive不会检验该路径是否有效。hive会加载指定路径下的所有文件。
  ```
* Hive创建内部表时，会将数据移动到数据仓库的指定路径下；若创建外部表，仅记录数据文件所在的存储路径，不对数据文件的位置做任何改变。在删除表的时候，内
  部表所对应的数据文件和元数据会被一起删除，而外部表只删除元数据，不删除数据文件。
  
#### 12.LIKE关键字与复制表结构
```
Like关键字允许用户复制现有表的表结构，但是不复制数据。
HQL语句示例： create [external] table [if not exist] [db_name.]table_name like existing_table;
```

#### 13.CLI常用显示命令
```
1.show tables：显示当前数据库的所有表
2.show databases|schemas：显示所有数据库
3.show partitions {?table_name}：显示表分区信息
4.show functions：显示当前版本hive支持的所有方法
5.desc extended {?table_name}：查看表信息
6.desc formatted {?table_name}：查看表信息（格式化后的,展示美观）
7.describe database {?database_name}：查看数据库相关信息
```  

#### 14.LOAD操作
```
1.在将数据加载到表中时，Hive不会进行任何转换。加载操作是将数据文件移动到与Hive表对应的位置的纯复制/移动操作。
2.语法结构：load data [local] inpath 'filepath' [overwrite] into table table_name [partiton(partcol1=val1,partcol2=val2,...)]
  其中，
  filepath可以是
  相对路径，例如： project/data1
  绝对路径，例如：/usr/hive/project/data1
  完整URI，例如：hdfs://namenode:9000/usr/hive/project/data1
  filepath可以引用一个文件(在这种情况下，hive将文件移动到表所对应的文件目录中)，或者是一个目录(在这种情况下，hive将把该目录中所有文件移动
  到表所对应的文件目录中)。
  local
  如果指定了local，load命令将在本地文件系统中查找文件路径，local指的是Hive服务端所在的机器。
  load命令会将filepath中的文件复制到目标文件系统中。目标文件系统由表的位置属性决定。被复制的数据文件移动到表的数据对应的位置。如果没有指定
  local关键字，且filepath指向的是一个完整的URI，hive会直接使用这个URI。否则，如果没有指定schema或者authority,hive会使用在hadoop配置文件
  中定义的schemas和authority，fs.default.name指定了NameNode的URI。
  overwrite
  如果使用了overwrite关键字，则目标表(或者分区)中的内容会被删除，然后再将filepath指向的文件/目录中的内容添加到表/分区中。如果目标表(或者分区)
  已经有一个文件，并且文件名和filepath中的文件名冲突，那么现有的文件会被新文件所替代。
```

#### 15.INSERT操作
```
1.Hive中insert主要是结合select查询语句使用，将查询结果插入到表中，例如：insert overwrite table t1 select * from t2； 。
2.需要保证查询结果列的数目和要插入的数据表的列的数目一致。
3.如果查询出来的数据类型和要插入的表的数据类型不一致，hive将会尝试进行转换，但是并不保证转换一定成功，转换失败的数据将会是NULL。
4.可以将一个表查询出来的数据插入到原表中，结果相当于自我复制了一份数据。
5.多重插入(Multi inserts)： 将表t1的id字段与name字段的值分别插入表t2与t3
      from t1 
      insert overwrite table t2 select id
      insert overwrite table t3 select name;
6.动态分区插入(Dynamic partition inserts)：动态分区是通过位置来指定分区值的。原始表select出来的值和输出partition的值的关系
  仅仅是通过位置来对应的，和HQL语句中的名称并没有关系。
7. 开启动态分区功能：set hive.exec.dynamic.partition=true, 默认是false；
   设置动态分区功能的模式：set hive.exec.dynamic.partition.mode=nonstrict, 默认是strict，表示必须指定至少一个分区为静态分区，
   nonstrict模式表示允许所有的分区字段都可以使用动态分区。
8.(1).创建目标表：create table t1(ip string) partitioned by(month string,day string);
  (2).动态插入分区：insert overwrite table t1 partition (month,day) select ip,substr(datee,1,7) as mon,datee from t2; 。 
```

#### 16.DIRECTORY关键字与导出表数据
```
HQL语法结构：
1.insert overwrite [local] directory 'path' select...from...,其中使用local时，表示导入到本地文件系统(Hive服务端所在的机器)，否则
  就是HDFS文件系统。
2.多重导出：将source表的数据分别导出到目录path1与path2
  from source
  insert overwrite [local] directory 'path1' select...from...
  insert overwrite [local] directory 'path2' select...from...
3.数据写入到文件系统时会进行文本序列化，且每列用^A来区分，\n为换行符。
```

#### 17.SELECT操作
```
基本select操作的语法结构：
    select [all | distinct] select_expr1,select_expr2,... 
    from table_name
    join table_other on expr
    [where where_condition]
    [group by col_list [having condition]]
    [cluster by col_list | [distribute by col_list][sort by | order by col_list]]
    [limit number]
说明：
1.order by会对输入做全局排序，因此只有一个reducer,会导致当输入规模较大时，需要较长的计算时间。
2.sort by 不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置 mapred.reduce.tasks>1,
  则 sort by 只保证每个reducer的输出有序，不保证全局有序。
3.distribute by根据指定字段将数据分到不同的reducer，分发算法是hash散列。
4.cluster by 除了具有distribute by的功能外，还会对该字段进行排序。
5.如果distribute by和sort by的是同一个字段时，此时， cluster by = distribute by + sort by。
```

#### 18.本地模式
```
在开发环境中，我们可以开启Hive的本地模式，以此来提高测试开发效率： set hive.exec.mode.local.auto=true 。
```

#### 19.JOIN操作
* Hive中除了支持传统数据库中的内关联、左关联、右关联与全关联外，还支持 LEFT SEMI JOIN 和 CROSS JOIN，但这两种类型的join也可以通过前面的几种替代。
* Hive支持等值连接(a.id=b.id)，不支持非等值(a.id>b.id)的连接，因为非等值连接非常难转化为 map/reduce 任务。另外，Hive支持2个以上表的连接。

#### 20.Hive参数配置
```
1.Hive命令行：输入 $HIVE_HOME/bin/hive -H 或者 -help 可以显示帮助选项。
  说明： -i    初始化 HQL 文件。
         -e   从命令行执行指定的 HQL，执行完毕后会继续回到Linux命令行
         -f   执行 HQL 脚本
         -v   输出执行的 HQL 语句到控制台
         -p <port>    通过指定的端口连接到Hive的服务端
         --hiveconf x=y    设置 hive/hadoop 的配置变量 x 的值为 y
  范例： $HIVE_HOME/bin/hive -e 'select * from table t'
        $HIVE_HOME/bin/hive -f /home/my/hive-script.sql
        $HIVE_HOME/bin/hive -f hdfs://<namenode>:<port>/hive-script.sql
        $HIVE_HOME/bin/hive -i /home/my/hive-init.sql
        $HIVE_HOME/bin/hive -e 'select col_list from table t' --hiveconf hive.exec.compress.output=true
                            --hiveconf mapred.reduce.tasks=32
2.Hive参数配置方式
  开发Hive应用时，不可避免地需要设定 Hive 的参数。设定 Hive 的参数可以调优HQL代码的执行效率，或帮助定位问题。
  然而实践中经常遇到的一个问题是，为什么设定的参数没有起作用？这通常是错误的设定方式导致的。
  对于一般参数，有以下三种设定方式：
      配置文件       全局有效
      命令行参数     对Hive启动实例有效
      参数声明       对hive的连接session有效
3.配置文件
  用户自定义配置文件  $HIVE_CONF_DIR/hive-site.xml     默认配置文件   $HIVE_CONF_DIR/hive-default.xml
  用户自定义配置会覆盖默认配置。另外，Hive 也会读入 Hadoop的配置，因为 Hive 是作为 Hadoop 的客户端启动的，Hive的配置会
  覆盖 Hadoop 的配置。配置文件设定的对本机启动的所有Hive进程都有效。
4.命令行参数
  启动 Hive(客户端或server方式)时，可以在命令行添加 --hiveconf 来设定参数。例如：bin/hive --hiveconf hive.root.logger=INFO.console,
  设定对本次的session（对于server 方式启动，则是所有请求的session）有效。
5.参数声明
  可以在 HQL 中使用 SET 关键字设定参数，这种方式设定的参数的作用域也是session级的。例如：set mapreduce.job.reduces=<num>,设置固定的
  reduce task的数量，但是，这个参数在必要时(业务逻辑决定只能用一个reduce task)，hive会忽略。
6.上述三种方式设定的参数的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，
  必须用前两种方式设定，因为这些参数的读取在session建立之前已经完成了。
```

#### 21.Hive运算符与内置函数
```
Hive有四种类型的运算符：关系运算符、逻辑运算符、算术运算符、复杂运算
```

#### 22.Hive自定义函数与Transform
```
1.自定义函数：当Hive提供的内置函数无法满足我们的业务处理需求时，此时就可以考虑使用用户自定义函数(UDF：user defined function)。
2.UDF开发实例：
  a).新建Java maven项目，添加hive-exec-1.2.1.jar和hadoop-common-2.7.4.jar依赖，然后编写一个java类，继承UDF，并重载evaluate方法。
  b).打成jar包上传到服务器。
  c).将jar包添加到hive的classpath：hive> add jar /usr/udf1.jar(jar包在服务器的位置)（在服务器所在机器的hive客户端执行该命令）
  d).创建临时函数与开发的java类的关联：hive> create temporary function <自定义的函数名称> as 'com.sx.hive.udf.MyUDF'，每次使用
     之前都应该先与开发的java类进行关联。
  e).此时，即可在hql中使用自定义的函数。
```






